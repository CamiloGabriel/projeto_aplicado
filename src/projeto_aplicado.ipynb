{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eb3e39-098e-483c-9bb2-d5127a6bf8d7",
   "metadata": {},
   "source": [
    "# Instalação e importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ec8da3-3c51-4afc-99cd-949940be1d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras) (13.8.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib scikit-learn opencv-python tensorflow keras opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1506d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 18:38:41.332586: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-29 18:38:41.334649: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 18:38:41.342959: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-29 18:38:41.360131: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 18:38:41.379536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 18:38:41.384001: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-29 18:38:41.405428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 18:38:42.307136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d1588-4b4f-48d2-a603-dc813b703855",
   "metadata": {},
   "source": [
    "**Atenção:** O aviso acima apenas informa que não foram encontrados drivers cuda e por conta disso será usada a CPU da máquina ao invés da GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28962c8a",
   "metadata": {},
   "source": [
    "# Coleta e processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1ed72e",
   "metadata": {},
   "outputs": [],
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_normalized = img_resized / 255.0\n",
    "    return img_normalized"
    "    return img_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e21c3d",
   "metadata": {},
   "source": [
    "### Criando CNN (rede neural convolucional)\n",
    "#### Essa função cria um modelo CNN que passa por várias camadas convolucionais para extrair características das imagens, nas quais foram utilizados a função ReLU (função de ativação que permite que a rede neural aprenda padrões complexos) e o Pooling (operação de amostragem usada em redes convolucionais para reduzir as dimensões e/ou tamanho da imagem ou das saídas das camadas convolucionais, porém, preservando suas características mais importantes) seguida por uma camada densa para classificação final em 3 classes, com a função de ativação softmax com 3 neurônios que transforma a saída em uma distribuição de probabilidades (somando 1), apropriada para tarefas de classificação multiclasses (neste caso, classificando em 3 classes possíveis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5430f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3_class_cnn(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=input_shape)) # Definie a forma de entrada\n",
    "\n",
    "    # Camada convolucional + ReLU + Pooling\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())  # Achata a saída para alimentar a camada densa\n",
    "    model.add(layers.Dense(128, activation='relu'))  # Camada densa\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_2_class_cnn(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=input_shape)) # Definie a forma de entrada\n",
    "\n",
    "    # Camada convolucional + ReLU + Pooling\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())  # Achata a saída para alimentar a camada densa\n",
    "    model.add(layers.Dense(128, activation='relu'))  # Camada densa\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "# Definindo a forma de entrada da imagem (altura, largura, canais)\n",
    "input_shape = (224, 224, 1)  # 1 canal para imagens em escala de cinza\n",
    "\n",
    "# Cria modelos propostos no projeto\n",
    "generalist_cnn_model = create_3_class_cnn(input_shape)\n",
    "lordosis_cnn_model = create_2_class_cnn(input_shape)\n",
    "kiphosis_cnn_model = create_2_class_cnn(input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885e959",
   "metadata": {},
   "source": [
    "# Pré-processamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf630b2-1805-4776-b3d6-32cb745ef3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerador de dados com aumentação de imagem\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    # mudar quando tiver dataSet para 0.2 ou 0.1\n",
    "    validation_split = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3832254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images belonging to 3 classes.\n",
      "Found 3 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "generalist_classes = ['healthy', 'kyphosis', 'lordosis']\n",
    "\n",
    "# Treinamento com as 3 classes: healthy, kiphosis, lordosis\n",
    "generalist_train_generator = train_datagen.flow_from_directory(\n",
    "    './data_set',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 1, # mudar para 8 quando tiver dataset completo\n",
    "    # aumenta o tamanho de processamento por vez, reduzindo peculiaridades de uma única imagem afetem a acurácia do modelo\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "generalist_validation_generator = train_datagen.flow_from_directory(\n",
    "    './data_set',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 1, # mudar para 8 quando tiver dataset completo\n",
    "    # aumenta o tamanho de processamento por vez, reduzindo peculiaridades de uma única imagem afetem a acurácia do modelo\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "# Compilando o modelo\n",
    "generalist_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd613387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n",
      "Found 2 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "lordosis_classes = ['healthy', 'lordosis']\n",
    "\n",
    "# Treinamento com as classes: healthy e lordosis\n",
    "lordosis_train_generator = train_datagen.flow_from_directory(\n",
    "    './data_set',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 1, # mudar para 8 quando tiver dataset completo\n",
    "    # aumenta o tamanho de processamento por vez, reduzindo peculiaridades de uma única imagem afetem a acurácia do modelo\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    classes = lordosis_classes  # Definindo as classes manualmente\n",
    ")\n",
    "\n",
    "lordosis_validation_generator = train_datagen.flow_from_directory(\n",
    "    './data_set',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 1, # mudar para 8 quando tiver dataset completo\n",
    "    # aumenta o tamanho de processamento por vez, reduzindo peculiaridades de uma única imagem afetem a acurácia do modelo\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation',\n",
    "    classes = lordosis_classes  # Definindo as classes manualmente\n",
    ")\n",
    "\n",
    "# Compilando o modelo\n",
    "lordosis_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e4f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n",
      "Found 2 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "kiphosis_classes = ['healthy', 'kyphosis']\n",
    "\n",
    "# Treinamento com as classes: healthy e kiphosis\n",
    "kiphosis_train_generator = train_datagen.flow_from_directory(\n",
    "    './data_set',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 1, # mudar para 8 quando tiver dataset completo\n",
    "    # aumenta o tamanho de processamento por vez, reduzindo peculiaridades de uma única imagem afetem a acurácia do modelo\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    classes = kiphosis_classes\n",
    ")\n",
    "\n",
    "kiphosis_validation_generator = train_datagen.flow_from_directory(\n",
    "    './data_set',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 1, # mudar para 8 quando tiver dataset completo\n",
    "    # aumenta o tamanho de processamento por vez, reduzindo peculiaridades de uma única imagem afetem a acurácia do modelo\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation',\n",
    "    classes = kiphosis_classes\n",
    ")\n",
    "\n",
    "# Compilando o modelo\n",
    "kiphosis_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06ed8b",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b34fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254ms/step - accuracy: 0.1667 - loss: 1.1192 - val_accuracy: 0.3333 - val_loss: 1.4018\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.8333 - loss: 0.5031 - val_accuracy: 0.0000e+00 - val_loss: 1.5664\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.5417 - loss: 1.1447 - val_accuracy: 0.3333 - val_loss: 1.3820\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.7083 - loss: 0.7849 - val_accuracy: 0.3333 - val_loss: 1.8875\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.6335 - val_accuracy: 0.3333 - val_loss: 0.9768\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.2917 - loss: 0.7279 - val_accuracy: 0.0000e+00 - val_loss: 1.5867\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.4829 - val_accuracy: 0.3333 - val_loss: 2.1754\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 208ms/step - accuracy: 0.4583 - loss: 0.7867 - val_accuracy: 0.3333 - val_loss: 1.8828\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5417 - loss: 0.5500 - val_accuracy: 0.6667 - val_loss: 1.9334\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step - accuracy: 0.2917 - loss: 1.0548 - val_accuracy: 0.6667 - val_loss: 1.8579\n",
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 422ms/step - accuracy: 0.6667 - loss: 0.7757 - val_accuracy: 0.5000 - val_loss: 0.8037\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 1.0000 - loss: 0.2607 - val_accuracy: 0.5000 - val_loss: 0.7046\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.6667 - loss: 0.8260 - val_accuracy: 0.5000 - val_loss: 0.8613\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.6667 - loss: 0.5828 - val_accuracy: 0.5000 - val_loss: 2.3322\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.6667 - loss: 2.4358 - val_accuracy: 1.0000 - val_loss: 0.3021\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.6667 - loss: 0.7902 - val_accuracy: 0.5000 - val_loss: 0.7851\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 0.3333 - loss: 0.8416 - val_accuracy: 0.5000 - val_loss: 0.7432\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.3333 - loss: 0.6967 - val_accuracy: 0.5000 - val_loss: 0.6767\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.6667 - loss: 0.7090 - val_accuracy: 1.0000 - val_loss: 0.6026\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.3333 - loss: 0.7071 - val_accuracy: 0.5000 - val_loss: 0.6288\n",
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 596ms/step - accuracy: 0.3333 - loss: 12436.8799 - val_accuracy: 0.5000 - val_loss: 918.9157\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.3333 - loss: 2484.2937 - val_accuracy: 0.5000 - val_loss: 418.6120\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.6667 - loss: 187.4506 - val_accuracy: 0.5000 - val_loss: 226.6048\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385ms/step - accuracy: 0.6667 - loss: 30.4564 - val_accuracy: 0.5000 - val_loss: 20.4187\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.3333 - loss: 42.2911 - val_accuracy: 0.5000 - val_loss: 3.5213\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.3333 - loss: 41.0618 - val_accuracy: 0.5000 - val_loss: 17.2539\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.6667 - loss: 16.1816 - val_accuracy: 0.5000 - val_loss: 29.0009\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.3333 - loss: 22.1032 - val_accuracy: 0.5000 - val_loss: 19.5984\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.0000e+00 - loss: 24.6046 - val_accuracy: 0.5000 - val_loss: 18.1922\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.6667 - loss: 22.1722 - val_accuracy: 0.5000 - val_loss: 7.8614\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo generalista\n",
    "generalist_history = generalist_cnn_model.fit(\n",
    "    generalist_train_generator,\n",
    "    validation_data = generalist_validation_generator,\n",
    "    epochs = 10\n",
    ")\n",
    "\n",
    "# Treinamento do modelo de lordose\n",
    "lordosis_history = lordosis_cnn_model.fit(\n",
    "    lordosis_train_generator,\n",
    "    validation_data = lordosis_validation_generator,\n",
    "    epochs = 10\n",
    ")\n",
    "\n",
    "# Treinamento do modelo de cifose\n",
    "kiphosis_history = kiphosis_cnn_model.fit(\n",
    "    kiphosis_train_generator,\n",
    "    validation_data = kiphosis_validation_generator,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b8cf4-89af-4aa1-bc2b-c4f160b02f93",
   "metadata": {},
   "source": [
    "### Ao inicializar o modelo os pesos da rede neural são inicializados de forma aleatória, e como o processo de otimização do modelo começa a partir de diferentes pontos cada execução pode levar a resultados diferentes, por isso, cada vez que o código for executado os resultados não serão exatamente os mesmos, mas aproximados.\n",
    "\n",
    "\n",
    "### image augmentation: técnicas de rotação, deslocamento, zoom e etc. aplicadas na imagem para criar novas variações para realizar o treinamento do modelo, sendo também um processo aleatório."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bfd3e",
   "metadata": {},
   "source": [
    "# Validação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a386ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7083 - loss: 2.0768 \n",
      "Validação modelo generalista - Loss: 2.2450571060180664, Acurácia: 0.6666666865348816\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.5303 \n",
      "Validação modelo de lordose - Loss: 0.5338822603225708, Acurácia: 1.0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6667 - loss: 6.8981     \n",
      "Validação modelo de cifose - Loss: 10.347177505493164, Acurácia: 0.5\n"
     ]
    }
   ],
   "source": [
    "generalist_val_loss, generalist_val_acc = generalist_cnn_model.evaluate(generalist_validation_generator)\n",
    "print(f\"Validação modelo generalista - Loss: {generalist_val_loss}, Acurácia: {generalist_val_acc}\")\n",
    "\n",
    "lordosis_val_loss, lordosis_val_acc = lordosis_cnn_model.evaluate(lordosis_validation_generator)\n",
    "print(f\"Validação modelo de lordose - Loss: {lordosis_val_loss}, Acurácia: {lordosis_val_acc}\")\n",
    "\n",
    "kiphosis_val_loss, kiphosis_val_acc = kiphosis_cnn_model.evaluate(kiphosis_validation_generator)\n",
    "print(f\"Validação modelo de cifose - Loss: {kiphosis_val_loss}, Acurácia: {kiphosis_val_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2017a",
   "metadata": {},
   "source": [
    "# Função de predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6ef124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Predição (modelo generalista): [[0.25545475 0.2638531  0.48069212]]\n",
      "Classe prevista (modelo generalista): lordosis\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Predição (modelo de lordose): [[0.49539304 0.50460696]]\n",
      "Classe prevista (modelo de lordose): lordosis\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Predição (modelo de cifose): [[0.5210334 0.4789666]]\n",
      "Classe prevista (modelo de cifose): healthy\n"
     ]
    }
   ],
   "source": [
    "def predict_image(image_path, model):\n",
    "    img = preprocess_image(image_path)\n",
    "    img = np.expand_dims(img, axis=0)  # Adiciona uma dimensão para batch\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction)  # Retorna o índice da classe com maior probabilidade\n",
    "    return predicted_class, prediction\n",
    "\n",
    "generalist_predicted_class, generalist_result = predict_image('./test-data/001.jpg', generalist_cnn_model)\n",
    "print(f'Predição (modelo generalista): {generalist_result}')\n",
    "print(f'Classe prevista (modelo generalista): {generalist_classes[generalist_predicted_class]}')\n",
    "\n",
    "lordosis_predicted_class, lordosis_result = predict_image('./test-data/001.jpg', lordosis_cnn_model)\n",
    "print(f'Predição (modelo de lordose): {lordosis_result}')\n",
    "print(f'Classe prevista (modelo de lordose): {lordosis_classes[lordosis_predicted_class]}')\n",
    "\n",
    "kiphosis_predicted_class, kiphosis_result = predict_image('./test-data/001.jpg', kiphosis_cnn_model)\n",
    "print(f'Predição (modelo de cifose): {kiphosis_result}')\n",
    "print(f'Classe prevista (modelo de cifose): {kiphosis_classes[kiphosis_predicted_class]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
